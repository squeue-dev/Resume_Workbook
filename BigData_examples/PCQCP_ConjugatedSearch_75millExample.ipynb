{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07KIvLJmkdYX"
      },
      "source": [
        "**Google Colab Notebook for the selection of compounds with conjugated $\\pi$ systems from PubChemQC Project's B3LYP/6-31G* Dataset**\n",
        "\n",
        "\n",
        "\n",
        "*   Author: Alfonso Esqueda García (esquedal94@gmail.com)\n",
        "\n",
        "Descripction:\n",
        "\n",
        "This script searches for chemical compounds that have certain attributes of interest from a public chemistry database stored in google drive (about 75 million molecules in the entiry database)\n",
        "\n",
        "\n",
        "For this Google Colab's notebook to work properly a shortcut of PubChemQC Project's b3lyp/6-31G* dataset in your Drive is needed. Follow this link to create said shortcut: \n",
        "\n",
        "https://drive.google.com/drive/u/0/folders/1N5q9p-H5TDggFWt8b0c26iwWL6WcGe6i \n",
        "\n",
        "\n",
        "-- PubChemQC Project website: http://pubchemqc.riken.jp/\n",
        "\n",
        "-- PubChemQC Project article: https://pubs.acs.org/doi/10.1021/acs.jcim.7b00083\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uCVGHO9uR76",
        "outputId": "003d040f-929c-4f51-a9c8-c4b9303cb5b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Load PubChemQC Project's B3LYP/6-31G* Dataset's Google Drive Shared Folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "PCQCP_dir = '/content/drive/MyDrive/b3lyp_JCIM2017/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2Ldx0hGrGFj",
        "outputId": "96b2419e-f697-45aa-e849-34261c6be7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.5 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.21.5)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.3.1\n",
            "Collecting openbabel-wheel\n",
            "  Downloading openbabel_wheel-3.1.1.5-cp37-cp37m-manylinux_2_24_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 26.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: openbabel-wheel\n",
            "Successfully installed openbabel-wheel-3.1.1.5\n"
          ]
        }
      ],
      "source": [
        "# Installation & importation of needed libraries\n",
        "# --Installs\n",
        "!pip install rdkit-pypi\n",
        "\n",
        "!pip install -U openbabel-wheel\n",
        "\n",
        "# --Imports\n",
        "from os import listdir, mkdir, rmdir, remove\n",
        "from os.path import isfile, isdir, join\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import shutil\n",
        "\n",
        "from multiprocessing import Pool, Lock \n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "import hashlib\n",
        "\n",
        "import json\n",
        "\n",
        "import tarfile # Library needed for the decompressing of .tar.gz files\n",
        "\n",
        "from rdkit import Chem # Chemical Toolbox Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muoItjxHzVcD"
      },
      "source": [
        "**Function Definitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ulZg91SoVfV"
      },
      "outputs": [],
      "source": [
        "# Global variables needed\n",
        "\n",
        "# Get as a list with every file in the Drive's shared folder that contains the\n",
        "# dataset\n",
        "ds_files = [f for f in listdir(PCQCP_dir) if isfile(join(PCQCP_dir, f))]\n",
        "dataset = [f for f in ds_files if '.md5' not in f]  # .md5 file will help to ve- \n",
        "                                                    # rify encryption signature\n",
        "                                                    # to assert the file's \n",
        "                                                    # integrity\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/PubChemQC_Project/'\n",
        "\n",
        "tmp_outdir = '/content/tmp_PCQCP/'\n",
        "try:\n",
        "    mkdir(tmp_outdir)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "cjtd_csv = '/content/drive/MyDrive/PubChemQC_Project/Conjugated_Molecules.csv'\n",
        "\n",
        "done_dict = output_dir + 'chunks_done.txt'\n",
        "\n",
        "# Definitions\n",
        "def iterate_DSChunks(id):\n",
        "    \"\"\"\n",
        "    Iterate the .tar.gz files that, combined, make up the complete B3LYP/6-31G*\n",
        "    dataset in the search of different chemical properties (Will be used to\n",
        "    look for molecules with conjugated pi systems)\n",
        "\n",
        "    int id : an index of the list containing all the .tar.gz files\n",
        "\n",
        "    return None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        chunk = dataset[id]\n",
        "    except IndexError:\n",
        "        return\n",
        "\n",
        "    lock1.acquire()\n",
        "    try:\n",
        "        with open(done_dict, 'r') as f:\n",
        "            chunks_done = json.load(f)\n",
        "    finally:\n",
        "        lock1.release()\n",
        "\n",
        "    if chunks_done[chunk]:\n",
        "        return\n",
        "    else:\n",
        "        # Lets verify the encryption signature of the file\n",
        "        chunk_compressed = PCQCP_dir + chunk\n",
        "        md5_file = chunk_compressed + '.md5'\n",
        "\n",
        "        original_md5 = ''\n",
        "        with open(md5_file, 'r') as f:\n",
        "            original_md5 = f.readline().split()[0]\n",
        "\n",
        "        with open(chunk_compressed, 'rb') as file_to_check:\n",
        "            # read contents of the file\n",
        "            data = file_to_check.read() \n",
        "\n",
        "            # pipe contents of the file through\n",
        "            md5_returned = hashlib.md5(data).hexdigest()\n",
        "\n",
        "            # Finally compare original MD5 with freshly calculated\n",
        "            if original_md5 != md5_returned:\n",
        "                return\n",
        "\n",
        "    # Extraction of the folder \n",
        "    tar_gz = tarfile.open(chunk_compressed)\n",
        "\n",
        "    # Directory that contains the extracted files\n",
        "    chunk_dir = tmp_outdir + chunk.replace('.tar.gz', '/') \n",
        "\n",
        "    # Extracting file\n",
        "    tar_gz.extractall(tmp_outdir)\n",
        "    tar_gz.close()\n",
        "\n",
        "    # Get a list with every folder that was extracted\n",
        "    compounds_dirs = [join(chunk_dir, o) for o in listdir(chunk_dir) \n",
        "                      if isdir(join(chunk_dir,o))]\n",
        "\n",
        "    # Loop for every compound directory\n",
        "    for compound in compounds_dirs:\n",
        "        molfiles = [f for f in listdir(compound) if (isfile(join(compound, f))\n",
        "                    and '.mol' in f)]\n",
        "\n",
        "        # Loop through every molfile to search for conjugated bonds\n",
        "        for mol in molfiles:\n",
        "            mol_fname = join(compound, mol)\n",
        "\n",
        "            try:\n",
        "                candidate_mol = Chem.MolFromMolFile(mol_fname)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Try to generate SMILE from molfile\n",
        "            try: \n",
        "                candidate_smi = Chem.MolToSmiles(candidate_mol)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Let's feed the SMILE to rdkit \n",
        "            rdkit_mol = Chem.MolFromSmiles(candidate_smi)\n",
        "\n",
        "            for bond in rdkit_mol.GetBonds():\n",
        "                if bond.GetIsConjugated():\n",
        "                    lock2.acquire() # Lock to prevent data overwrite\n",
        "                    try:\n",
        "                        with open(cjtd_csv, 'a') as f:\n",
        "                            f.write('\\n')\n",
        "                            nline = str(candidate_smi) + ', ' + str(mol_fname)\n",
        "                            f.write(nline)\n",
        "                    finally:\n",
        "                        lock2.release()\n",
        "                        \n",
        "                    # Copy molfile to output directory\n",
        "                    #dst_name = molfiles_dir + mol\n",
        "                    #shutil.copyfile(mol_fname, dst_name)\n",
        "\n",
        "                    # Get out of bond iterating loop\n",
        "                    break\n",
        "                else: \n",
        "                    continue\n",
        "         \n",
        "        # Compound's bonds search ends here, let's erase the compound dir\n",
        "        try:\n",
        "            shutil.rmtree(compound) # remove compound dir\n",
        "        except shutil.Error:\n",
        "            pass\n",
        "\n",
        "    # When every compound is searched erase the tmp_folder created\n",
        "    try:\n",
        "        shutil.rmtree(chunk_dir) # remove chunk dir\n",
        "    except shutil.Error:\n",
        "        pass\n",
        "    \n",
        "    lock1.acquire()\n",
        "    try:\n",
        "        with open(done_dict, 'r') as f:\n",
        "            chunks_done = json.load(f)\n",
        "        \n",
        "        chunks_done[chunk] = True\n",
        "        \n",
        "        with open(done_dict, 'w') as f:\n",
        "            json.dump(chunks_done, f)\n",
        "    finally:\n",
        "        lock1.release()\n",
        "\n",
        "    lock3.acquire()\n",
        "    try:\n",
        "        logfile = output_dir + 'progress.log'\n",
        "\n",
        "        with open(logfile, 'w') as f:\n",
        "            done_ctr = Counter(chunks_done.values())[True]\n",
        "            nof_chunks = len(chunks_done)\n",
        "            buff = '\\r Progreso: ' + str(done_ctr) + ' chunks analizados de '\n",
        "            buff += str(nof_chunks) + ' ----- '  \n",
        "            buff += str(round(((float(done_ctr)/nof_chunks) * 100), 1)) + '%'\n",
        "            f.write(buff)\n",
        "    finally:\n",
        "        lock3.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4PMOaVszgvq"
      },
      "source": [
        "**---------------------------------------------**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWZ19QaH0iqg"
      },
      "source": [
        "**- Run the iterate_DSChunks function with multiple parallel threads**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8JZDe5H5ENm"
      },
      "outputs": [],
      "source": [
        "def init(l1, l2, l3):\n",
        "    global lock1\n",
        "    global lock2\n",
        "    global lock3\n",
        "    \n",
        "    lock1 = l1\n",
        "    lock2 = l2\n",
        "    lock3 = l3\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    iter = [i for i in range(0, len(dataset))]\n",
        "\n",
        "    l1 = Lock()\n",
        "    l2 = Lock()\n",
        "    l3 = Lock()\n",
        "\n",
        "    with Pool(7, initializer=init, initargs=(l1, l2, l3,)) as p:\n",
        "        p.map(iterate_DSChunks, iter)\n",
        "        p.close()\n",
        "        p.join()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXECUTE = False"
      ],
      "metadata": {
        "id": "3xBTELO84Xl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N_HvhqSUHoj"
      },
      "outputs": [],
      "source": [
        "# Before iterating through every compound in the dataset we need to populate a  \n",
        "# dictionary with values 'done' or 'not done' (True or False) for practicity for \n",
        "# every chunk in the dataset folder \n",
        "\n",
        "# THE FOLLOWING CODE ONLY NEEDS TO BE RUN ONCE. Its purpose is to create for the\n",
        "# first time the done_dict file (the value for every key will be False ('not\n",
        "# done))\n",
        "if EXECUTE:\n",
        "    fresh_dict = {}\n",
        "\n",
        "    for chunk in dataset:\n",
        "        fresh_dict[chunk] = False\n",
        "\n",
        "    with open(done_dict, 'w') as f:    \n",
        "        json.dump(fresh_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MANUALLY erase the content of the tmp_PCQCP folder \n",
        "if EXECUTE:\n",
        "    !rm -rf /content/tmp_PCQCP/* # This might only work in Google Colab"
      ],
      "metadata": {
        "id": "Bt0Psgsk49-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MANUALLY erase the content of the Conjugated Molecules CSV\n",
        "\n",
        "if EXECUTE:\n",
        "    # This might only work in Google Colab\n",
        "    !rm  /content/drive/MyDrive/PubChemQC_Project/Conjugated_Molecules.csv\n",
        "\n",
        "    with open(cjtd_csv, 'w') as f:\n",
        "        f.write('SMILE, PATH')  "
      ],
      "metadata": {
        "id": "RXyxVJ2j_AFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PCQCP_ConjugatedSearch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}